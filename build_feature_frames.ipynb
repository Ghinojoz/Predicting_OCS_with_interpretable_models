{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"build_feature_frames.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMzstSHqvLj38rm+VKsCkx0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dHsQnkVwMgWg"},"outputs":[],"source":["!apt-get install -qq libgdal-dev libproj-dev\n","!pip install --no-binary shapely shapely --force\n","!pip install cartopy\n","!pip install regionmask\n","\n","\n","#!pip install regionmask\n","#!pip install cartopy\n","\n","#!pip install -q --upgrade ipython\n","#!pip install -q --upgrade ipykernel"]},{"cell_type":"code","source":["#import required packages\n","import os\n","import warnings\n","import time\n","import regionmask\n","import gc\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import xarray as xr\n","import geopandas as gpd\n","import cartopy.crs as ccrs\n","import shapely\n","from datetime import datetime as dt\n","from shapely.geometry import Point\n","from shapely.geometry.polygon import Polygon\n","from scipy.interpolate import interp1d\n","from dateutil.relativedelta import relativedelta\n","from google.colab import drive\n","from google.colab import files"],"metadata":{"id":"_fGjRiyBMp2V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#establish working directory and mount drive\n","drive.mount('/content/drive')\n","working_directory = '/content/drive/My Drive/COS Seesaw Research'"],"metadata":{"id":"6KaSN7bEMshc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Parallel site arrays for constructing regions, and time delta for building feature time series in addition, contains the radius for defining map regions"],"metadata":{"id":"j4wC7gGNSuoO"}},{"cell_type":"code","source":["cos_sites = ['alt', 'brw', 'cgo', 'hfm', 'kum', 'lef', 'mhd', 'mlo', 'nwr', 'psa', 'smo', 'spo', 'sum', 'thd']\n","cos_site_centers = [(-62.3, 82.5), (-156.6, 71.3), (144.7,-40.7), (-72.2, 42.5), (-154.8, 19.5), (-90.3, 45.6), (-9.9, 53.3), (-155.6, 19.5), (-105.5, 40.1), (-64.0, -64.6), (-170.6, -14.2), (0, -90), (-38.4, 72.6), (-124.1,41.0)]\n","time_delta_general = [('-15d', relativedelta(days=-15)), ('-1m', relativedelta(months=-1)), ('-1m15d', relativedelta(months=-1, days=-15)), ('-2m', relativedelta(months=-2))]\n","region_size = 30\n","year_start = 2000\n","year_end = 2018\n","divider = ('-------------------------------------------------------------------------------------------------------')"],"metadata":{"id":"biTKC3leMvg8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Build regions"],"metadata":{"id":"ui5EmgycS2wW"}},{"cell_type":"code","source":["regions = None\n","names = []\n","abbrevs = []\n","region_list = []\n","\n","for i in range(len(cos_sites)):\n","  names.append(cos_sites[i])\n","  abbrevs.append(cos_sites[i])\n","  center_point = Point(cos_site_centers[i][0], cos_site_centers[i][1])\n","  circle = center_point.buffer(region_size)\n","\n","  #region_bound = np.array([list(cos_site_centers[i])])\n","  \n","  #region_list.append(region_bound)\n","  region_list.append(circle)\n","\n","regions = regionmask.Regions(region_list, names=names, abbrevs=abbrevs, name='Ocean Regions', overlap=True)\n","plt.figure(figsize=(24,12))\n","regions.plot(label='abbrev')\n","plt.show()"],"metadata":{"id":"wGK3bqFAzsmZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Establish dictionary of different data frames, one for each of the observation sites"],"metadata":{"id":"_rqiqzslStEv"}},{"cell_type":"code","source":["days = pd.date_range(start=dt(year=2005, month=1, day=1), end=dt(year=2017, month=1, day=1), freq='SM')\n","days_frame = pd.DataFrame({'time':days})"],"metadata":{"id":"VElHZjtySQMg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Helper function for getting regionalized data"],"metadata":{"id":"MKtrtVuodJMI"}},{"cell_type":"code","source":["def standardize_longitude(data):\n","    print(divider)\n","    print('Standardizing Longitude')\n","    print(divider)\n","    data = data.assign_coords(lon = (((data.lon + 180) % 360) -180))\n","    data = data.sortby(data.lon)\n","    print('Done standardizing longitude')\n","    print(divider)\n","    return data"],"metadata":{"id":"dlFYltHm5AlG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def standardize_time_Month(data, year, tName_start, tName_end):\n","    print(divider)\n","    print('Standardizing time')\n","    print(divider)\n","    dates = []\n","    for val in data[tName_start].values:\n","        date = dt(year=year, month=int(val), day=1)\n","        dates.append(date)\n","    data = data.rename({tName_start : tName_end})\n","    data = data.assign_coords(time=dates)\n","    print('Done standardizing time')\n","    print(divider)\n","    return data"],"metadata":{"id":"Inh5yOgn40Af"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_regionalized_map_data(file_name_start, file_name_end, variable_name, regions, start, end, standardizeLon=True, standardizeTime=None):\n","  data_dict = {}\n","  for region in regions:\n","    data_dict[region.abbrev] = []\n","  \n","  for year in range(start, end + 1):\n","    f_name = file_name_start + str(year) + file_name_end\n","    data = None\n","    # Some of the downward solar radiation data used a date time format that\n","    # xarray did not like, this attempts to manually convert it, but may need to be\n","    # adjusted for future issues\n","    try:\n","        data = xr.open_dataset(f_name).load()\n","    except:\n","        print(\"Error opening \" + f_name)\n","        print(\"Attempting alternative datetime conversion\")\n","        try:\n","            data = xr.open_dataset(f_name, decode_times=False).load()\n","            time_since = dt(year=1800, month=1, day=1)\n","            temp_time = data['time'].to_series()\n","            funct = lambda x : relativedelta(hours=x)\n","            time_col = temp_time.apply(funct)\n","            funct2 = lambda x: time_since + x\n","            data['time'] = time_col.apply(funct2)\n","            print(\"Success!\")\n","        except Exception as e:\n","            print(\"Failed\")\n","            print(e)\n","            exit()\n","          \n","    # If necessary, convert 0 to 360 to -180 to 180\n","    if standardizeLon:\n","        data = standardize_longitude(data)\n","\n","    if standardizeTime is not None:\n","        data = standardize_time_Month(data, year, standardizeTime[0], standardizeTime[1])\n","\n","    # get the mean for each region\n","    #data_mask = regions.mask(data)\n","    data_mask = regions.mask_3D(data)\n","    for region in regions:\n","        region_index = regions.map_keys(region.name)\n","        region_mask = data_mask.sel(region=region_index)\n","        region_data = data.where(region_mask)\n","        data_mean = region_data.mean(dim=('lat', 'lon'))\n","        data_dict[region.abbrev].append(data_mean)\n","\n","    print('Region Data added')\n","    data.close()\n","    gc.collect()\n","\n","\n","  # combine each region's yearly mean into single data set\n","  for region in data_dict.keys():\n","      data_dict[region] = xr.concat(data_dict[region], dim='time')\n","\n","  return data_dict\n","  "],"metadata":{"id":"kxL6hcv6dI9X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Helper function for interpolating using cubic"],"metadata":{"id":"FLsoS4k90_R8"}},{"cell_type":"code","source":["def interp_data(region_var, interp_dates):\n","  interp = region_var.interp(time=list(map(str, interp_dates)), method='cubic')\n","  return interp"],"metadata":{"id":"4H1BXobR0_DF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Add features to region dataframes"],"metadata":{"id":"e0k1rl-mbRbc"}},{"cell_type":"code","source":["offset_dates = []\n","for delta in time_delta_general:\n","  funct = lambda x: x + delta[1]\n","  current_offset = days_frame['time'].apply(funct)\n","  offset_dates.append((delta[0], current_offset))"],"metadata":{"id":"ci-LoftdbHPX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Add SST -- disabled to see if we can survive mask 3d on data with different granularity"],"metadata":{"id":"NI6XcQOLWS-a"}},{"cell_type":"code","source":["# add sea surface temperature data\n","\n","f_name = working_directory + '/Data/sst/sst.day.mean.'\n","sst_dict = get_regionalized_map_data(f_name, '.nc', '_sst', regions, year_start, year_end)\n","\n","for region in sst_dict.keys():\n","  interp = interp_data(sst_dict[region].sst, days_frame['time'])\n","  column_name = region + '_sst'\n","  days_frame[column_name] = interp\n","\n","  for delta in offset_dates:\n","    column_name = region + '_sst' + delta[0]\n","    interp = interp_data(sst_dict[region].sst, delta[1])\n","    days_frame[column_name] = interp\n","  \n","days_frame = days_frame.copy()\n"],"metadata":{"id":"EhEGl2k6toXF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Add CDOM"],"metadata":{"id":"b1AwLGb-A5e-"}},{"cell_type":"code","source":["f_name = working_directory + '/Data/CDOM/CDOM_a350_'\n","cdom_dict = get_regionalized_map_data(f_name, '.nc', '_cdom', regions, year_start, year_end, standardizeTime=('month', 'time'))\n","for region in cdom_dict.keys():\n","  interp = interp_data(cdom_dict[region].CDOM_a350, days_frame['time'])\n","  column_name = region + '_cdom'\n","  days_frame[column_name] = interp\n","\n","  for delta in offset_dates:\n","    column_name = region + '_cdom' + delta[0]\n","    interp = interp_data(cdom_dict[region].CDOM_a350, delta[1])\n","    days_frame[column_name] = interp\n","      \n","days_frame = days_frame.copy()"],"metadata":{"id":"6wlICWia_Zru"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Add downward radiation"],"metadata":{"id":"Aasn0T77A_Xl"}},{"cell_type":"code","source":["f_name = working_directory + '/Data/DSRF/dswrf.sfc.gauss.'\n","dswrf_dict = get_regionalized_map_data(f_name, '.nc', 'dswrf', regions, year_start, year_end)\n","\n","for region in dswrf_dict.keys():\n","  interp = interp_data(dswrf_dict[region].dswrf, days_frame['time'])\n","  column_name = region + '_dswrf'\n","  days_frame[column_name] = interp\n","\n","  for delta in offset_dates:\n","    column_name = region + '_dswrf' + delta[0]\n","    interp = interp_data(dswrf_dict[region].dswrf, delta[1])\n","    days_frame[column_name] = interp\n","      \n","days_frame = days_frame.copy()"],"metadata":{"id":"iloEmqWnA-3N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["add v wind"],"metadata":{"id":"ZifAUkwJB_xF"}},{"cell_type":"code","source":["f_name = working_directory + '/Data/VWND/vwnd.10m.gauss.'\n","vwnd_dict = get_regionalized_map_data(f_name, '.nc', 'vwnd', regions, year_start, year_end)\n","for region in vwnd_dict.keys():\n","  interp = interp_data(vwnd_dict[region].vwnd, days_frame['time'])\n","  column_name = region + '_vwnd'\n","  days_frame[column_name] = interp\n","\n","  for delta in offset_dates:\n","    column_name = region + '_vwnd' + delta[0]\n","    interp = interp_data(vwnd_dict[region].vwnd, delta[1])\n","    days_frame[column_name] = interp\n","\n","days_frame = days_frame.copy()"],"metadata":{"id":"-HVvK_ekB--1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["add uwnd"],"metadata":{"id":"NReHCuP9Ct_4"}},{"cell_type":"code","source":["f_name = working_directory + '/Data/UWND/uwnd.10m.gauss.'\n","uwnd_dict = get_regionalized_map_data(f_name, '.nc', 'uwnd', regions, year_start, year_end)\n","for region in uwnd_dict.keys():\n","  interp = interp_data(uwnd_dict[region].uwnd, days_frame['time'])\n","  column_name = region + '_uwnd'\n","  days_frame[column_name] = interp\n","\n","  for delta in offset_dates:\n","    column_name = region + '_uwnd' + delta[0]\n","    interp = interp_data(uwnd_dict[region].uwnd, delta[1])\n","    days_frame[column_name] = interp\n","\n","days_frame = days_frame.copy()"],"metadata":{"id":"4lAxDoQ4CvRF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#let's copy and normalize \n","normalized_df = days_frame.copy()\n","feature_columns = normalized_df.columns.tolist()\n","if 'time' in feature_columns:\n","  feature_columns.remove('time')\n","\n","print(feature_columns)"],"metadata":{"id":"rmgMKjyQErUT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for column in feature_columns:\n","  normalized_df[column] = (normalized_df[column] - normalized_df[column].min()) / (normalized_df[column].max() - normalized_df[column].min())\n","\n","display(normalized_df)"],"metadata":{"id":"LWfyg2t2F1m6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save non-normalized\n","if days_frame.isnull().values.any():\n","  total_null = days_frame.isnull().sum().sum()\n","  print('Total null values: ', total_null)\n","\n","days_frame.dropna(inplace=True)\n","pickle_name = working_directory + '/Data/Pickles/correlation_frame.pkl'\n","days_frame.to_pickle(pickle_name)"],"metadata":{"id":"zji6SO5PG295"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save normalized_df\n","if normalized_df.isnull().values.any():\n","  total_null = normalized_df.isnull.sum().sum()\n","  print('Total null values: ', total_null)\n","\n","normalized_df.dropna(inplace=True)\n","pickle_name = working_directory + '/Data/Pickles/correlation_frame_normalized.pkl'\n","normalized_df.to_pickle(pickle_name)"],"metadata":{"id":"QkHFIPnwQ0yG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Clean up and save pickles"],"metadata":{"id":"2QpqnIGkC9YN"}},{"cell_type":"code","source":["drive.flush_and_unmount()"],"metadata":{"id":"i5d1pZEQDnNd"},"execution_count":null,"outputs":[]}]}